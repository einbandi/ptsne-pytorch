{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import datetime\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import numpy as np\r\n",
    "from numba import jit\r\n",
    "from scipy.spatial.distance import pdist, squareform\r\n",
    "from scipy.optimize import root_scalar, curve_fit\r\n",
    "from pynndescent import NNDescent\r\n",
    "from scipy.sparse import csr_matrix\r\n",
    "from sklearn.decomposition import PCA"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def exp_k(dists, sigma):\r\n",
    "    return np.exp(- (dists - dists[0]) / sigma).sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def find_sigma(d, k, lower_bound=1e-6, upper_bound=1e6):\r\n",
    "    return root_scalar(\r\n",
    "        lambda s: exp_k(d, s) - np.log2(k),\r\n",
    "        bracket=(lower_bound, upper_bound)\r\n",
    "    ).root"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def p_ij_sym(x, k, verbose=False):\r\n",
    "    num_pts = x.shape[0]\r\n",
    "    if verbose:\r\n",
    "        print('Indexing')\r\n",
    "    index = NNDescent(x)\r\n",
    "    neighbors = np.empty((num_pts, k), dtype=np.int)\r\n",
    "    p_ij = np.empty((num_pts, k))\r\n",
    "    for i, xi in enumerate(x):\r\n",
    "        if verbose:\r\n",
    "            print('Calculating probabilities: {cur}/{tot}'.format(\r\n",
    "                cur=i+1, tot=num_pts), end='\\r')\r\n",
    "        nn, dists = index.query([xi], k+1)\r\n",
    "        sigma = find_sigma(dists[0, 1:], k)\r\n",
    "        neighbors[i] = nn[0, 1:]\r\n",
    "        p_ij[i] = np.exp(- (dists[0, 1:] - dists[0, 1]) / sigma)\r\n",
    "    row_indices = np.repeat(np.arange(num_pts), k)\r\n",
    "    p = csr_matrix((p_ij.ravel(), (row_indices, neighbors.ravel())))\r\n",
    "    return p + p.transpose() - (p.multiply(p.transpose()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def pdiff(x):\r\n",
    "    batch_size = x.shape[0]\r\n",
    "    expanded = x.unsqueeze(1)\r\n",
    "    tiled = torch.repeat_interleave(expanded, batch_size, dim=1)\r\n",
    "    diffs = tiled - tiled.transpose(0, 1)  \r\n",
    "    return diffs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def find_ab_params(spread, min_dist):\r\n",
    "    \"\"\"Fit a, b params for the differentiable curve used in lower\r\n",
    "    dimensional fuzzy simplicial complex construction. We want the\r\n",
    "    smooth curve (from a pre-defined family with simple gradient) that\r\n",
    "    best matches an offset exponential decay.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def curve(x, a, b):\r\n",
    "        return 1.0 / (1.0 + a * x ** (2 * b))\r\n",
    "\r\n",
    "    xv = np.linspace(0, spread * 3, 300)\r\n",
    "    yv = np.zeros(xv.shape)\r\n",
    "    yv[xv < min_dist] = 1.0\r\n",
    "    yv[xv >= min_dist] = np.exp(-(xv[xv >= min_dist] - min_dist) / spread)\r\n",
    "    params, covar = curve_fit(curve, xv, yv)\r\n",
    "    return params[0], params[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def dist_mat_squared(x):\r\n",
    "    batch_size = x.shape[0]\r\n",
    "    expanded = x.unsqueeze(1)\r\n",
    "    tiled = torch.repeat_interleave(expanded, batch_size, dim=1)\r\n",
    "    diffs = tiled - tiled.transpose(0, 1)\r\n",
    "    sum_act = torch.sum(torch.pow(diffs,2), axis=2)    \r\n",
    "    return sum_act"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "# more memory-efficient version of distmat\r\n",
    "def dist_mat_squared_new(x):\r\n",
    "    n = x.shape[0]\r\n",
    "    distmat_cond = F.pdist(x)\r\n",
    "    distmat = torch.zeros((n,n), device=x.device)\r\n",
    "    a,b = torch.triu_indices(n, n, offset=1)\r\n",
    "    distmat[[a,b]] = distmat_cond.pow(2)\r\n",
    "    return distmat + distmat.T"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def q_ij(x, a, b):\r\n",
    "    dists = dist_mat_squared_new(x)\r\n",
    "    dists.fill_diagonal_(1.)\r\n",
    "    q = torch.pow(1 + a * torch.pow(dists, b), -1)\r\n",
    "    q.fill_diagonal_(0.)\r\n",
    "    return q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def submatrix(m, indices):\r\n",
    "    dim = len(indices)\r\n",
    "    indices = np.array(np.meshgrid(indices, indices)).T.reshape(-1,2).T\r\n",
    "    return torch.tensor(m[indices[0], indices[1]].reshape(dim, dim))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def umap_loss(p, q, eps=1.0e-7):\r\n",
    "    eps = torch.tensor(eps, dtype=p.dtype)\r\n",
    "    l_a = torch.mul(p, torch.log(p + eps) - torch.log(q + eps))\r\n",
    "    l_b = torch.mul(1 - p, torch.log(1 - p + eps) - torch.log(1 - q + eps))\r\n",
    "    loss_mat = l_a #+ l_b\r\n",
    "    loss_mat.fill_diagonal_(0.)\r\n",
    "    return torch.sum(loss_mat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Parametric_UMAP(nn.Module):\r\n",
    "    \r\n",
    "    def __init__(self, input_dim, output_dim, knn, min_dist,\r\n",
    "                 spread=1.0,\r\n",
    "                 hidden_layer_dims=None,\r\n",
    "                 seed=None,\r\n",
    "                 use_cuda=False):\r\n",
    "        super().__init__()\r\n",
    "        \r\n",
    "        self.input_dim = input_dim\r\n",
    "        self.output_dim = output_dim\r\n",
    "        self.knn = knn\r\n",
    "        self.min_dist = min_dist\r\n",
    "        self.spread = spread\r\n",
    "        \r\n",
    "        self.a, self.b = find_ab_params(self.spread, self.min_dist)\r\n",
    "        \r\n",
    "        self.use_cuda = use_cuda\r\n",
    "        \r\n",
    "        if seed is not None:\r\n",
    "            torch.manual_seed(seed)\r\n",
    "            np.random.seed(seed)\r\n",
    "        \r\n",
    "        # If no layers provided, use the same architecture as van der maaten 2009 paper\r\n",
    "        if hidden_layer_dims is None:\r\n",
    "            hidden_layer_dims = [500, 500, 2000]\r\n",
    "        \r\n",
    "        self.layers = nn.ModuleList()\r\n",
    "        \r\n",
    "        cur_dim = input_dim\r\n",
    "        for hdim in hidden_layer_dims:\r\n",
    "            self.layers.append(nn.Linear(cur_dim, hdim))\r\n",
    "            cur_dim = hdim\r\n",
    "        self.layers.append(nn.Linear(cur_dim, output_dim))\r\n",
    "        \r\n",
    "        if self.use_cuda:\r\n",
    "            self.cuda()\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        for layer in self.layers[:-1]:\r\n",
    "            # x = torch.sigmoid(layer(x))\r\n",
    "            x = F.softplus(layer(x))\r\n",
    "        out = self.layers[-1](x)\r\n",
    "        return out\r\n",
    "    \r\n",
    "    def pretrain(self, training_data,\r\n",
    "            epochs=10,\r\n",
    "            verbose=False,\r\n",
    "            batch_size=500,\r\n",
    "            learning_rate=0.01):\r\n",
    "        if verbose:\r\n",
    "            print('Calculating PCA')\r\n",
    "        pca = torch.tensor(PCA(n_components=2).fit_transform(training_data), dtype=training_data.dtype)\r\n",
    "        \r\n",
    "        dataset = torch.utils.data.TensorDataset(training_data, pca)\r\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\r\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\r\n",
    "        criterion = nn.MSELoss()\r\n",
    "        \r\n",
    "        if verbose:\r\n",
    "            print('{time}: Beginning pretraining for {epochs} epochs'.format(\r\n",
    "                time=datetime.datetime.now(),\r\n",
    "                epochs=epochs))\r\n",
    "        \r\n",
    "        for epoch in range(epochs):\r\n",
    "            running_loss = 0\r\n",
    "            for batch, data in enumerate(dataloader):\r\n",
    "                \r\n",
    "                features, targets = data\r\n",
    "                    \r\n",
    "                if self.use_cuda:\r\n",
    "                    features = features.cuda()\r\n",
    "                    targets = targets.cuda()\r\n",
    "                    \r\n",
    "                optim.zero_grad()\r\n",
    "                \r\n",
    "                loss = criterion(self(features), targets)\r\n",
    "                \r\n",
    "                loss.backward()\r\n",
    "                optim.step()\r\n",
    "                    \r\n",
    "                running_loss += loss.item()\r\n",
    "                \r\n",
    "            if verbose:\r\n",
    "                print('{time}: Loss after epoch {ep}: {rloss}'.format(\r\n",
    "                    time=datetime.datetime.now(),\r\n",
    "                    ep=epoch,\r\n",
    "                    rloss=running_loss))\r\n",
    "\r\n",
    "        if verbose:\r\n",
    "            print('{time}: Finished pretraining'.format(\r\n",
    "                time=datetime.datetime.now()))\r\n",
    "        \r\n",
    "    def fit(self, training_data,\r\n",
    "            p_ij=None,\r\n",
    "            pretrain=False,\r\n",
    "            epochs=10,\r\n",
    "            verbose=False,\r\n",
    "            optimizer=torch.optim.Adam,\r\n",
    "            batch_size=500,\r\n",
    "            learning_rate=0.01):\r\n",
    "        \r\n",
    "        assert training_data.shape[1] == self.input_dim, \"Input training data must be same shape as training `num_inputs`\"\r\n",
    "        \r\n",
    "        self.p_ij = p_ij\r\n",
    "        self._epochs = epochs\r\n",
    "        \r\n",
    "        if pretrain:\r\n",
    "            self.pretrain(training_data, epochs=5, verbose=verbose, batch_size=batch_size)\r\n",
    "            \r\n",
    "        if self.p_ij is None:\r\n",
    "            self.p_ij = p_ij_sym(training_data.numpy(), self.knn, verbose=verbose)\r\n",
    "            \r\n",
    "        dataset = torch.utils.data.TensorDataset(training_data, torch.arange(training_data.shape[0]))\r\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\r\n",
    "        optim = optimizer(self.parameters(), lr=learning_rate)\r\n",
    "        \r\n",
    "        if verbose:\r\n",
    "            print('{time}: Beginning training for {epochs} epochs'.format(\r\n",
    "                time=datetime.datetime.now(),\r\n",
    "                epochs=epochs))\r\n",
    "        \r\n",
    "        for epoch in range(epochs):\r\n",
    "            running_loss = 0\r\n",
    "            for batch, data in enumerate(dataloader):\r\n",
    "                \r\n",
    "                features, indices = data\r\n",
    "                \r\n",
    "                p = submatrix(self.p_ij, indices.numpy())\r\n",
    "                    \r\n",
    "                if self.use_cuda:\r\n",
    "                    features = features.cuda()\r\n",
    "                    p = p.cuda()\r\n",
    "                    \r\n",
    "                optim.zero_grad()\r\n",
    "                \r\n",
    "                q = q_ij(self(features), self.a, self.b)\r\n",
    "                loss = umap_loss(p, q)\r\n",
    "                \r\n",
    "                loss.backward()\r\n",
    "                optim.step()\r\n",
    "                    \r\n",
    "                running_loss += loss.item()\r\n",
    "                \r\n",
    "            if verbose:\r\n",
    "                print('{time}: Loss after epoch {ep}: {rloss}'.format(\r\n",
    "                    time=datetime.datetime.now(),\r\n",
    "                    ep=epoch+1,\r\n",
    "                    rloss=running_loss))\r\n",
    "\r\n",
    "        if verbose:\r\n",
    "            print('{time}: Finished training'.format(\r\n",
    "                time=datetime.datetime.now()))\r\n",
    "            \r\n",
    "    def test(self, training_data,\r\n",
    "            p_ij=None,\r\n",
    "            optimizer=torch.optim.Adam,\r\n",
    "            batch_size=500,\r\n",
    "            learning_rate=0.01):\r\n",
    "        \r\n",
    "        assert training_data.shape[1] == self.input_dim, \"Input training data must be same shape as training `num_inputs`\"\r\n",
    "        \r\n",
    "        self.p_ij = p_ij\r\n",
    "            \r\n",
    "        if self.p_ij is None:\r\n",
    "            self.p_ij = p_ij_sym(training_data.numpy(), self.knn, verbose=True)\r\n",
    "            \r\n",
    "        dataset = torch.utils.data.TensorDataset(training_data, torch.arange(training_data.shape[0]))\r\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\r\n",
    "        optim = optimizer(self.parameters(), lr=learning_rate)\r\n",
    "        \r\n",
    "        for epoch in range(5):\r\n",
    "            running_loss = 0\r\n",
    "            for batch, data in enumerate(dataloader):\r\n",
    "                \r\n",
    "                features, indices = data\r\n",
    "                \r\n",
    "                p = submatrix(self.p_ij, indices.numpy())\r\n",
    "                    \r\n",
    "                if self.use_cuda:\r\n",
    "                    features = features.cuda()\r\n",
    "                    p = p.cuda()\r\n",
    "                    \r\n",
    "                optim.zero_grad()\r\n",
    "                \r\n",
    "                q = q_ij(self(features), self.a, self.b)\r\n",
    "                loss = umap_loss(p, q)\r\n",
    "                \r\n",
    "                return self(features), p, q, loss\r\n",
    "                \r\n",
    "                loss.backward()\r\n",
    "                optim.step()\r\n",
    "                    \r\n",
    "                running_loss += loss.item()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "-----------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mnist = MNIST(\n",
    "    './data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "testdata = mnist.data.reshape(-1, 28*28) / 255."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "foo = Parametric_UMAP(28*28, 2, 100, 0.01, use_cuda=True, hidden_layer_dims=[300,100])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "p_precalc = foo.p_ij"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "foo.fit(testdata[:20000], p_ij=p_precalc, batch_size=500, epochs=20, learning_rate=0.1, pretrain=False, verbose=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bar = foo(testdata[:10000].cuda()).cpu().detach().numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "colors = [plt.cm.tab10.colors[i] for i in mnist.targets[:10000]]\n",
    "ax.scatter(bar[:,0],bar[:,1],c=colors, s=2)\n",
    "recs = []\n",
    "for i in range(0,10):\n",
    "    recs.append(mpatches.Rectangle((0,0),1,1,fc=plt.cm.tab10.colors[i]))\n",
    "ax.legend(recs,list(range(10)),loc=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "foof, foop, fooq, fool = foo.test(testdata[:20000], p_ij=p_precalc, batch_size=500, learning_rate=0.1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "foop.min()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a = foop.detach().cpu()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "b = foof.clone().detach().cpu().requires_grad_(True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "c = q_ij(b, 1.93, 0.79)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "c"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(a/c).fill_diagonal_(0.).min()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "b.grad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a = torch.tensor([[0.5, 0.6], [0.7, 0.3]])\n",
    "b = torch.tensor([[0.1, 0.2], [0.4, 0.3]], requires_grad=True)\n",
    "c = q_ij(b, 1.93, 0.79)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "umap_loss(a, c).backward()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "b.grad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b28ae10d83720f36e64a475947a69598f522ce7ea0d14cd27597289d37d6b09"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}