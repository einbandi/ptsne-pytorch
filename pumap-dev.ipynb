{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import datetime\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import numpy as np\r\n",
    "from numba import jit\r\n",
    "from scipy.spatial.distance import pdist, squareform\r\n",
    "from scipy.optimize import root_scalar, curve_fit\r\n",
    "from pynndescent import NNDescent\r\n",
    "from scipy.sparse import csr_matrix\r\n",
    "from sklearn.decomposition import PCA"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def exp_k(dists, sigma):\r\n",
    "    return np.exp(- (dists - dists[0]) / sigma).sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def find_sigma(d, k, lower_bound=1e-6, upper_bound=1e6):\r\n",
    "    return root_scalar(\r\n",
    "        lambda s: exp_k(d, s) - np.log2(k),\r\n",
    "        bracket=(lower_bound, upper_bound)\r\n",
    "    ).root"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def p_ij_sym(x, k, verbose=False):\r\n",
    "    num_pts = x.shape[0]\r\n",
    "    if verbose:\r\n",
    "        print('Indexing')\r\n",
    "    index = NNDescent(x)\r\n",
    "    neighbors = np.empty((num_pts, k), dtype=np.int)\r\n",
    "    p_ij = np.empty((num_pts, k))\r\n",
    "    for i, xi in enumerate(x):\r\n",
    "        if verbose:\r\n",
    "            print('Calculating probabilities: {cur}/{tot}'.format(\r\n",
    "                cur=i+1, tot=num_pts), end='\\r')\r\n",
    "        nn, dists = index.query([xi], k+1)\r\n",
    "        sigma = find_sigma(dists[0, 1:], k)\r\n",
    "        neighbors[i] = nn[0, 1:]\r\n",
    "        p_ij[i] = np.exp(- (dists[0, 1:] - dists[0, 1]) / sigma)\r\n",
    "    row_indices = np.repeat(np.arange(num_pts), k)\r\n",
    "    p = csr_matrix((p_ij.ravel(), (row_indices, neighbors.ravel())))\r\n",
    "    return p + p.transpose() - (p.multiply(p.transpose()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def pdiff(x):\r\n",
    "    batch_size = x.shape[0]\r\n",
    "    expanded = x.unsqueeze(1)\r\n",
    "    tiled = torch.repeat_interleave(expanded, batch_size, dim=1)\r\n",
    "    diffs = tiled - tiled.transpose(0, 1)  \r\n",
    "    return diffs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def find_ab_params(spread, min_dist):\r\n",
    "    \"\"\"Fit a, b params for the differentiable curve used in lower\r\n",
    "    dimensional fuzzy simplicial complex construction. We want the\r\n",
    "    smooth curve (from a pre-defined family with simple gradient) that\r\n",
    "    best matches an offset exponential decay.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def curve(x, a, b):\r\n",
    "        return 1.0 / (1.0 + a * x ** (2 * b))\r\n",
    "\r\n",
    "    xv = np.linspace(0, spread * 3, 300)\r\n",
    "    yv = np.zeros(xv.shape)\r\n",
    "    yv[xv < min_dist] = 1.0\r\n",
    "    yv[xv >= min_dist] = np.exp(-(xv[xv >= min_dist] - min_dist) / spread)\r\n",
    "    params, covar = curve_fit(curve, xv, yv)\r\n",
    "    return params[0], params[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def dist_mat_squared(x):\r\n",
    "    batch_size = x.shape[0]\r\n",
    "    expanded = x.unsqueeze(1)\r\n",
    "    tiled = torch.repeat_interleave(expanded, batch_size, dim=1)\r\n",
    "    diffs = tiled - tiled.transpose(0, 1)\r\n",
    "    sum_act = torch.sum(torch.pow(diffs,2), axis=2)    \r\n",
    "    return sum_act"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "# more memory-efficient version of distmat\r\n",
    "def dist_mat_squared_new(x):\r\n",
    "    n = x.shape[0]\r\n",
    "    distmat_cond = F.pdist(x)\r\n",
    "    distmat = torch.zeros((n,n), device=x.device)\r\n",
    "    a,b = torch.triu_indices(n, n, offset=1)\r\n",
    "    distmat[[a,b]] = distmat_cond.pow(2)\r\n",
    "    return distmat + distmat.T"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "source": [
    "foo = torch.tensor([[-1,-3],[-1,1],[1,5],[4,-1]], dtype=torch.float32, requires_grad=True)\r\n",
    "states = [foo.clone().detach()]\r\n",
    "for _ in range(100):\r\n",
    "    bar = 1 / dist_mat_squared_new(foo).sum()\r\n",
    "    bar.backward()\r\n",
    "    with torch.no_grad():\r\n",
    "        foo += foo.grad\r\n",
    "    states.append(foo.clone().detach())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "source": [
    "states"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[tensor([[-1., -3.],\n",
       "         [-1.,  1.],\n",
       "         [ 1.,  5.],\n",
       "         [ 4., -1.]]),\n",
       " tensor([[-0.9998, -2.9997],\n",
       "         [-0.9998,  1.0000],\n",
       "         [ 1.0000,  4.9996],\n",
       "         [ 3.9997, -0.9999]]),\n",
       " tensor([[-0.9995, -2.9990],\n",
       "         [-0.9995,  0.9999],\n",
       "         [ 0.9999,  4.9987],\n",
       "         [ 3.9991, -0.9996]]),\n",
       " tensor([[-0.9990, -2.9980],\n",
       "         [-0.9990,  0.9997],\n",
       "         [ 0.9999,  4.9975],\n",
       "         [ 3.9982, -0.9992]]),\n",
       " tensor([[-0.9984, -2.9967],\n",
       "         [-0.9984,  0.9995],\n",
       "         [ 0.9998,  4.9958],\n",
       "         [ 3.9970, -0.9986]]),\n",
       " tensor([[-0.9975, -2.9951],\n",
       "         [-0.9975,  0.9993],\n",
       "         [ 0.9996,  4.9937],\n",
       "         [ 3.9954, -0.9979]]),\n",
       " tensor([[-0.9966, -2.9931],\n",
       "         [-0.9966,  0.9990],\n",
       "         [ 0.9995,  4.9912],\n",
       "         [ 3.9936, -0.9971]]),\n",
       " tensor([[-0.9954, -2.9908],\n",
       "         [-0.9954,  0.9987],\n",
       "         [ 0.9993,  4.9882],\n",
       "         [ 3.9915, -0.9961]]),\n",
       " tensor([[-0.9941, -2.9882],\n",
       "         [-0.9941,  0.9983],\n",
       "         [ 0.9992,  4.9849],\n",
       "         [ 3.9891, -0.9950]]),\n",
       " tensor([[-0.9926, -2.9853],\n",
       "         [-0.9926,  0.9979],\n",
       "         [ 0.9989,  4.9811],\n",
       "         [ 3.9863, -0.9937]]),\n",
       " tensor([[-0.9910, -2.9820],\n",
       "         [-0.9910,  0.9974],\n",
       "         [ 0.9987,  4.9768],\n",
       "         [ 3.9833, -0.9923]]),\n",
       " tensor([[-0.9892, -2.9784],\n",
       "         [-0.9892,  0.9969],\n",
       "         [ 0.9985,  4.9722],\n",
       "         [ 3.9799, -0.9907]]),\n",
       " tensor([[-0.9872, -2.9744],\n",
       "         [-0.9872,  0.9963],\n",
       "         [ 0.9982,  4.9671],\n",
       "         [ 3.9762, -0.9890]]),\n",
       " tensor([[-0.9851, -2.9701],\n",
       "         [-0.9851,  0.9957],\n",
       "         [ 0.9979,  4.9616],\n",
       "         [ 3.9723, -0.9872]]),\n",
       " tensor([[-0.9828, -2.9655],\n",
       "         [-0.9828,  0.9951],\n",
       "         [ 0.9975,  4.9557],\n",
       "         [ 3.9680, -0.9852]]),\n",
       " tensor([[-0.9803, -2.9606],\n",
       "         [-0.9803,  0.9944],\n",
       "         [ 0.9972,  4.9493],\n",
       "         [ 3.9634, -0.9831]]),\n",
       " tensor([[-0.9776, -2.9553],\n",
       "         [-0.9776,  0.9936],\n",
       "         [ 0.9968,  4.9425],\n",
       "         [ 3.9585, -0.9808]]),\n",
       " tensor([[-0.9748, -2.9497],\n",
       "         [-0.9748,  0.9928],\n",
       "         [ 0.9964,  4.9353],\n",
       "         [ 3.9532, -0.9784]]),\n",
       " tensor([[-0.9718, -2.9437],\n",
       "         [-0.9718,  0.9920],\n",
       "         [ 0.9960,  4.9276],\n",
       "         [ 3.9477, -0.9759]]),\n",
       " tensor([[-0.9687, -2.9374],\n",
       "         [-0.9687,  0.9911],\n",
       "         [ 0.9955,  4.9195],\n",
       "         [ 3.9418, -0.9732]]),\n",
       " tensor([[-0.9654, -2.9307],\n",
       "         [-0.9654,  0.9901],\n",
       "         [ 0.9951,  4.9109],\n",
       "         [ 3.9357, -0.9703]]),\n",
       " tensor([[-0.9618, -2.9237],\n",
       "         [-0.9618,  0.9891],\n",
       "         [ 0.9945,  4.9019],\n",
       "         [ 3.9291, -0.9673]]),\n",
       " tensor([[-0.9582, -2.9163],\n",
       "         [-0.9582,  0.9880],\n",
       "         [ 0.9940,  4.8924],\n",
       "         [ 3.9223, -0.9641]]),\n",
       " tensor([[-0.9543, -2.9086],\n",
       "         [-0.9543,  0.9869],\n",
       "         [ 0.9935,  4.8825],\n",
       "         [ 3.9152, -0.9608]]),\n",
       " tensor([[-0.9503, -2.9006],\n",
       "         [-0.9503,  0.9858],\n",
       "         [ 0.9929,  4.8722],\n",
       "         [ 3.9077, -0.9574]]),\n",
       " tensor([[-0.9461, -2.8922],\n",
       "         [-0.9461,  0.9846],\n",
       "         [ 0.9923,  4.8613],\n",
       "         [ 3.8999, -0.9538]]),\n",
       " tensor([[-0.9417, -2.8834],\n",
       "         [-0.9417,  0.9833],\n",
       "         [ 0.9917,  4.8501],\n",
       "         [ 3.8917, -0.9500]]),\n",
       " tensor([[-0.9371, -2.8742],\n",
       "         [-0.9371,  0.9820],\n",
       "         [ 0.9910,  4.8383],\n",
       "         [ 3.8832, -0.9461]]),\n",
       " tensor([[-0.9324, -2.8647],\n",
       "         [-0.9324,  0.9807],\n",
       "         [ 0.9903,  4.8261],\n",
       "         [ 3.8744, -0.9420]]),\n",
       " tensor([[-0.9274, -2.8549],\n",
       "         [-0.9274,  0.9793],\n",
       "         [ 0.9896,  4.8134],\n",
       "         [ 3.8652, -0.9378]]),\n",
       " tensor([[-0.9223, -2.8446],\n",
       "         [-0.9223,  0.9778],\n",
       "         [ 0.9889,  4.8002],\n",
       "         [ 3.8557, -0.9334]]),\n",
       " tensor([[-0.9170, -2.8340],\n",
       "         [-0.9170,  0.9763],\n",
       "         [ 0.9881,  4.7866],\n",
       "         [ 3.8459, -0.9289]]),\n",
       " tensor([[-0.9115, -2.8230],\n",
       "         [-0.9115,  0.9747],\n",
       "         [ 0.9874,  4.7725],\n",
       "         [ 3.8357, -0.9242]]),\n",
       " tensor([[-0.9058, -2.8116],\n",
       "         [-0.9058,  0.9731],\n",
       "         [ 0.9865,  4.7578],\n",
       "         [ 3.8251, -0.9193]]),\n",
       " tensor([[-0.8999, -2.7999],\n",
       "         [-0.8999,  0.9714],\n",
       "         [ 0.9857,  4.7427],\n",
       "         [ 3.8142, -0.9142]]),\n",
       " tensor([[-0.8939, -2.7877],\n",
       "         [-0.8939,  0.9697],\n",
       "         [ 0.9848,  4.7271],\n",
       "         [ 3.8029, -0.9090]]),\n",
       " tensor([[-0.8876, -2.7752],\n",
       "         [-0.8876,  0.9679],\n",
       "         [ 0.9839,  4.7110],\n",
       "         [ 3.7912, -0.9037]]),\n",
       " tensor([[-0.8811, -2.7622],\n",
       "         [-0.8811,  0.9660],\n",
       "         [ 0.9830,  4.6943],\n",
       "         [ 3.7792, -0.8981]]),\n",
       " tensor([[-0.8744, -2.7489],\n",
       "         [-0.8744,  0.9641],\n",
       "         [ 0.9821,  4.6772],\n",
       "         [ 3.7668, -0.8924]]),\n",
       " tensor([[-0.8676, -2.7351],\n",
       "         [-0.8676,  0.9622],\n",
       "         [ 0.9811,  4.6595],\n",
       "         [ 3.7541, -0.8865]]),\n",
       " tensor([[-0.8605, -2.7210],\n",
       "         [-0.8605,  0.9601],\n",
       "         [ 0.9801,  4.6413],\n",
       "         [ 3.7409, -0.8804]]),\n",
       " tensor([[-0.8532, -2.7064],\n",
       "         [-0.8532,  0.9581],\n",
       "         [ 0.9790,  4.6225],\n",
       "         [ 3.7274, -0.8742]]),\n",
       " tensor([[-0.8457, -2.6914],\n",
       "         [-0.8457,  0.9559],\n",
       "         [ 0.9780,  4.6032],\n",
       "         [ 3.7134, -0.8677]]),\n",
       " tensor([[-0.8380, -2.6759],\n",
       "         [-0.8380,  0.9537],\n",
       "         [ 0.9769,  4.5833],\n",
       "         [ 3.6991, -0.8611]]),\n",
       " tensor([[-0.8300, -2.6600],\n",
       "         [-0.8300,  0.9514],\n",
       "         [ 0.9757,  4.5629],\n",
       "         [ 3.6843, -0.8543]]),\n",
       " tensor([[-0.8219, -2.6437],\n",
       "         [-0.8219,  0.9491],\n",
       "         [ 0.9746,  4.5419],\n",
       "         [ 3.6692, -0.8473]]),\n",
       " tensor([[-0.8135, -2.6269],\n",
       "         [-0.8135,  0.9467],\n",
       "         [ 0.9734,  4.5204],\n",
       "         [ 3.6536, -0.8401]]),\n",
       " tensor([[-0.8049, -2.6097],\n",
       "         [-0.8049,  0.9442],\n",
       "         [ 0.9721,  4.4982],\n",
       "         [ 3.6376, -0.8327]]),\n",
       " tensor([[-0.7960, -2.5920],\n",
       "         [-0.7960,  0.9417],\n",
       "         [ 0.9709,  4.4754],\n",
       "         [ 3.6211, -0.8251]]),\n",
       " tensor([[-0.7869, -2.5738],\n",
       "         [-0.7869,  0.9391],\n",
       "         [ 0.9696,  4.4521],\n",
       "         [ 3.6043, -0.8174]]),\n",
       " tensor([[-0.7776, -2.5552],\n",
       "         [-0.7776,  0.9365],\n",
       "         [ 0.9682,  4.4281],\n",
       "         [ 3.5869, -0.8094]]),\n",
       " tensor([[-0.7680, -2.5360],\n",
       "         [-0.7680,  0.9337],\n",
       "         [ 0.9669,  4.4035],\n",
       "         [ 3.5692, -0.8012]]),\n",
       " tensor([[-0.7582, -2.5164],\n",
       "         [-0.7582,  0.9309],\n",
       "         [ 0.9655,  4.3782],\n",
       "         [ 3.5509, -0.7927]]),\n",
       " tensor([[-0.7481, -2.4962],\n",
       "         [-0.7481,  0.9280],\n",
       "         [ 0.9640,  4.3523],\n",
       "         [ 3.5322, -0.7841]]),\n",
       " tensor([[-0.7378, -2.4755],\n",
       "         [-0.7378,  0.9251],\n",
       "         [ 0.9625,  4.3257],\n",
       "         [ 3.5130, -0.7752]]),\n",
       " tensor([[-0.7272, -2.4543],\n",
       "         [-0.7272,  0.9220],\n",
       "         [ 0.9610,  4.2984],\n",
       "         [ 3.4933, -0.7661]]),\n",
       " tensor([[-0.7163, -2.4326],\n",
       "         [-0.7163,  0.9189],\n",
       "         [ 0.9595,  4.2705],\n",
       "         [ 3.4731, -0.7568]]),\n",
       " tensor([[-0.7051, -2.4103],\n",
       "         [-0.7051,  0.9158],\n",
       "         [ 0.9579,  4.2418],\n",
       "         [ 3.4524, -0.7473]]),\n",
       " tensor([[-0.6937, -2.3874],\n",
       "         [-0.6937,  0.9125],\n",
       "         [ 0.9562,  4.2124],\n",
       "         [ 3.4311, -0.7375]]),\n",
       " tensor([[-0.6820, -2.3639],\n",
       "         [-0.6820,  0.9091],\n",
       "         [ 0.9546,  4.1822],\n",
       "         [ 3.4094, -0.7274]]),\n",
       " tensor([[-0.6699, -2.3399],\n",
       "         [-0.6699,  0.9057],\n",
       "         [ 0.9528,  4.1513],\n",
       "         [ 3.3870, -0.7171]]),\n",
       " tensor([[-0.6576, -2.3152],\n",
       "         [-0.6576,  0.9022],\n",
       "         [ 0.9511,  4.1195],\n",
       "         [ 3.3641, -0.7065]]),\n",
       " tensor([[-0.6450, -2.2899],\n",
       "         [-0.6450,  0.8986],\n",
       "         [ 0.9493,  4.0870],\n",
       "         [ 3.3406, -0.6957]]),\n",
       " tensor([[-0.6320, -2.2640],\n",
       "         [-0.6320,  0.8949],\n",
       "         [ 0.9474,  4.0537],\n",
       "         [ 3.3165, -0.6846]]),\n",
       " tensor([[-0.6187, -2.2374],\n",
       "         [-0.6187,  0.8911],\n",
       "         [ 0.9455,  4.0195],\n",
       "         [ 3.2918, -0.6732]]),\n",
       " tensor([[-0.6050, -2.2101],\n",
       "         [-0.6050,  0.8872],\n",
       "         [ 0.9436,  3.9844],\n",
       "         [ 3.2665, -0.6615]]),\n",
       " tensor([[-0.5910, -2.1821],\n",
       "         [-0.5910,  0.8832],\n",
       "         [ 0.9416,  3.9484],\n",
       "         [ 3.2405, -0.6495]]),\n",
       " tensor([[-0.5767, -2.1534],\n",
       "         [-0.5767,  0.8791],\n",
       "         [ 0.9395,  3.9115],\n",
       "         [ 3.2138, -0.6372]]),\n",
       " tensor([[-0.5619, -2.1239],\n",
       "         [-0.5619,  0.8748],\n",
       "         [ 0.9374,  3.8736],\n",
       "         [ 3.1865, -0.6245]]),\n",
       " tensor([[-0.5468, -2.0936],\n",
       "         [-0.5468,  0.8705],\n",
       "         [ 0.9353,  3.8347],\n",
       "         [ 3.1584, -0.6116]]),\n",
       " tensor([[-0.5313, -2.0626],\n",
       "         [-0.5313,  0.8661],\n",
       "         [ 0.9330,  3.7948],\n",
       "         [ 3.1296, -0.5983]]),\n",
       " tensor([[-0.5154, -2.0307],\n",
       "         [-0.5154,  0.8615],\n",
       "         [ 0.9308,  3.7538],\n",
       "         [ 3.1000, -0.5846]]),\n",
       " tensor([[-0.4990, -1.9980],\n",
       "         [-0.4990,  0.8569],\n",
       "         [ 0.9284,  3.7117],\n",
       "         [ 3.0696, -0.5706]]),\n",
       " tensor([[-0.4822, -1.9643],\n",
       "         [-0.4822,  0.8520],\n",
       "         [ 0.9260,  3.6684],\n",
       "         [ 3.0383, -0.5561]]),\n",
       " tensor([[-0.4649, -1.9298],\n",
       "         [-0.4649,  0.8471],\n",
       "         [ 0.9236,  3.6240],\n",
       "         [ 3.0062, -0.5413]]),\n",
       " tensor([[-0.4471, -1.8942],\n",
       "         [-0.4471,  0.8420],\n",
       "         [ 0.9210,  3.5783],\n",
       "         [ 2.9732, -0.5261]]),\n",
       " tensor([[-0.4288, -1.8576],\n",
       "         [-0.4288,  0.8368],\n",
       "         [ 0.9184,  3.5312],\n",
       "         [ 2.9392, -0.5104]]),\n",
       " tensor([[-0.4100, -1.8200],\n",
       "         [-0.4100,  0.8314],\n",
       "         [ 0.9157,  3.4828],\n",
       "         [ 2.9043, -0.4943]]),\n",
       " tensor([[-0.3906, -1.7812],\n",
       "         [-0.3906,  0.8259],\n",
       "         [ 0.9129,  3.4330],\n",
       "         [ 2.8683, -0.4777]]),\n",
       " tensor([[-0.3706, -1.7413],\n",
       "         [-0.3706,  0.8202],\n",
       "         [ 0.9101,  3.3816],\n",
       "         [ 2.8312, -0.4605]]),\n",
       " tensor([[-0.3500, -1.7001],\n",
       "         [-0.3500,  0.8143],\n",
       "         [ 0.9071,  3.3287],\n",
       "         [ 2.7929, -0.4429]]),\n",
       " tensor([[-0.3288, -1.6576],\n",
       "         [-0.3288,  0.8082],\n",
       "         [ 0.9041,  3.2740],\n",
       "         [ 2.7535, -0.4247]]),\n",
       " tensor([[-0.3068, -1.6137],\n",
       "         [-0.3068,  0.8020],\n",
       "         [ 0.9010,  3.2176],\n",
       "         [ 2.7127, -0.4059]]),\n",
       " tensor([[-0.2841, -1.5683],\n",
       "         [-0.2841,  0.7955],\n",
       "         [ 0.8977,  3.1592],\n",
       "         [ 2.6706, -0.3864]]),\n",
       " tensor([[-0.2607, -1.5213],\n",
       "         [-0.2607,  0.7888],\n",
       "         [ 0.8944,  3.0989],\n",
       "         [ 2.6269, -0.3663]]),\n",
       " tensor([[-0.2363, -1.4727],\n",
       "         [-0.2363,  0.7818],\n",
       "         [ 0.8909,  3.0363],\n",
       "         [ 2.5818, -0.3454]]),\n",
       " tensor([[-0.2111, -1.4222],\n",
       "         [-0.2111,  0.7746],\n",
       "         [ 0.8873,  2.9714],\n",
       "         [ 2.5349, -0.3238]]),\n",
       " tensor([[-0.1849, -1.3697],\n",
       "         [-0.1849,  0.7671],\n",
       "         [ 0.8836,  2.9039],\n",
       "         [ 2.4862, -0.3013]]),\n",
       " tensor([[-0.1576, -1.3151],\n",
       "         [-0.1576,  0.7593],\n",
       "         [ 0.8797,  2.8337],\n",
       "         [ 2.4355, -0.2779]]),\n",
       " tensor([[-0.1291, -1.2582],\n",
       "         [-0.1291,  0.7512],\n",
       "         [ 0.8756,  2.7605],\n",
       "         [ 2.3826, -0.2535]]),\n",
       " tensor([[-0.0993, -1.1986],\n",
       "         [-0.0993,  0.7427],\n",
       "         [ 0.8713,  2.6840],\n",
       "         [ 2.3273, -0.2280]]),\n",
       " tensor([[-0.0681, -1.1363],\n",
       "         [-0.0681,  0.7338],\n",
       "         [ 0.8669,  2.6038],\n",
       "         [ 2.2694, -0.2013]]),\n",
       " tensor([[-0.0353, -1.0707],\n",
       "         [-0.0353,  0.7244],\n",
       "         [ 0.8622,  2.5195],\n",
       "         [ 2.2085, -0.1732]]),\n",
       " tensor([[-7.4622e-04, -1.0015e+00],\n",
       "         [-7.4622e-04,  7.1450e-01],\n",
       "         [ 8.5725e-01,  2.4305e+00],\n",
       "         [ 2.1442e+00, -1.4350e-01]]),\n",
       " tensor([[ 0.0359, -0.9282],\n",
       "         [ 0.0359,  0.7040],\n",
       "         [ 0.8520,  2.3362],\n",
       "         [ 2.0761, -0.1121]]),\n",
       " tensor([[ 0.0750, -0.8500],\n",
       "         [ 0.0750,  0.6929],\n",
       "         [ 0.8464,  2.2357],\n",
       "         [ 2.0036, -0.0786]]),\n",
       " tensor([[ 0.1169, -0.7662],\n",
       "         [ 0.1169,  0.6809],\n",
       "         [ 0.8404,  2.1279],\n",
       "         [ 1.9257, -0.0426]]),\n",
       " tensor([[ 0.1623, -0.6754],\n",
       "         [ 0.1623,  0.6679],\n",
       "         [ 0.8340,  2.0113],\n",
       "         [ 1.8415, -0.0038]]),\n",
       " tensor([[ 0.2120, -0.5761],\n",
       "         [ 0.2120,  0.6537],\n",
       "         [ 0.8269,  1.8835],\n",
       "         [ 1.7492,  0.0388]]),\n",
       " tensor([[ 0.2673, -0.4655],\n",
       "         [ 0.2673,  0.6379],\n",
       "         [ 0.8190,  1.7413],\n",
       "         [ 1.6465,  0.0862]]),\n",
       " tensor([[ 0.3303, -0.3393],\n",
       "         [ 0.3303,  0.6199],\n",
       "         [ 0.8100,  1.5791],\n",
       "         [ 1.5294,  0.1403]])]"
      ]
     },
     "metadata": {},
     "execution_count": 213
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "source": [
    "bar = dist_mat_squared_new(foo).sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "source": [
    "bar.backward()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "source": [
    "foo.grad"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ -400.3861,    47.3844, -1921.0607,  ...,  -362.8801,  -951.4847,\n",
       "          -382.1129],\n",
       "        [ 1370.8921, -1097.1067, -1099.6359,  ...,  -404.0282,  -260.7700,\n",
       "          1126.6843],\n",
       "        [ -278.9686,  1723.1650, -1009.9611,  ...,  1556.6245,  1477.4880,\n",
       "         -1001.4337],\n",
       "        ...,\n",
       "        [ 1258.7163,   886.4594,   506.8771,  ...,  -253.6815,  1898.3467,\n",
       "          -921.6090],\n",
       "        [-1330.0966,  1862.8799, -1584.3247,  ...,  1622.6505,    51.9634,\n",
       "         -1540.7065],\n",
       "        [-1740.8436,   995.6321,  -321.3471,  ...,  1029.9807,  1579.5074,\n",
       "          -137.0587]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 190
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "source": [
    "foo.grad"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ -400.3863,    47.3844, -1921.0609,  ...,  -362.8799,  -951.4847,\n",
       "          -382.1129],\n",
       "        [ 1370.8921, -1097.1067, -1099.6355,  ...,  -404.0281,  -260.7701,\n",
       "          1126.6844],\n",
       "        [ -278.9687,  1723.1642, -1009.9618,  ...,  1556.6246,  1477.4889,\n",
       "         -1001.4344],\n",
       "        ...,\n",
       "        [ 1258.7168,   886.4589,   506.8768,  ...,  -253.6816,  1898.3466,\n",
       "          -921.6089],\n",
       "        [-1330.0967,  1862.8804, -1584.3248,  ...,  1622.6510,    51.9635,\n",
       "         -1540.7072],\n",
       "        [-1740.8440,   995.6320,  -321.3473,  ...,  1029.9811,  1579.5079,\n",
       "          -137.0588]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 197
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def q_ij(x, a, b):\r\n",
    "    dists = dist_mat_squared(x)\r\n",
    "    dists.fill_diagonal_(1.)\r\n",
    "    q = torch.pow(1 + a * torch.pow(dists, b), -1)\r\n",
    "    q.fill_diagonal_(0.)\r\n",
    "    return q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def submatrix(m, indices):\r\n",
    "    dim = len(indices)\r\n",
    "    indices = np.array(np.meshgrid(indices, indices)).T.reshape(-1,2).T\r\n",
    "    return torch.tensor(m[indices[0], indices[1]].reshape(dim, dim))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def umap_loss(p, q, eps=1.0e-7):\r\n",
    "    eps = torch.tensor(eps, dtype=p.dtype)\r\n",
    "    l_a = torch.mul(p, torch.log(p + eps) - torch.log(q + eps))\r\n",
    "    l_b = torch.mul(1 - p, torch.log(1 - p + eps) - torch.log(1 - q + eps))\r\n",
    "    loss_mat = l_a #+ l_b\r\n",
    "    loss_mat.fill_diagonal_(0.)\r\n",
    "    return torch.sum(loss_mat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Parametric_UMAP(nn.Module):\r\n",
    "    \r\n",
    "    def __init__(self, input_dim, output_dim, knn, min_dist,\r\n",
    "                 spread=1.0,\r\n",
    "                 hidden_layer_dims=None,\r\n",
    "                 seed=None,\r\n",
    "                 use_cuda=False):\r\n",
    "        super().__init__()\r\n",
    "        \r\n",
    "        self.input_dim = input_dim\r\n",
    "        self.output_dim = output_dim\r\n",
    "        self.knn = knn\r\n",
    "        self.min_dist = min_dist\r\n",
    "        self.spread = spread\r\n",
    "        \r\n",
    "        self.a, self.b = find_ab_params(self.spread, self.min_dist)\r\n",
    "        \r\n",
    "        self.use_cuda = use_cuda\r\n",
    "        \r\n",
    "        if seed is not None:\r\n",
    "            torch.manual_seed(seed)\r\n",
    "            np.random.seed(seed)\r\n",
    "        \r\n",
    "        # If no layers provided, use the same architecture as van der maaten 2009 paper\r\n",
    "        if hidden_layer_dims is None:\r\n",
    "            hidden_layer_dims = [500, 500, 2000]\r\n",
    "        \r\n",
    "        self.layers = nn.ModuleList()\r\n",
    "        \r\n",
    "        cur_dim = input_dim\r\n",
    "        for hdim in hidden_layer_dims:\r\n",
    "            self.layers.append(nn.Linear(cur_dim, hdim))\r\n",
    "            cur_dim = hdim\r\n",
    "        self.layers.append(nn.Linear(cur_dim, output_dim))\r\n",
    "        \r\n",
    "        if self.use_cuda:\r\n",
    "            self.cuda()\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        for layer in self.layers[:-1]:\r\n",
    "            # x = torch.sigmoid(layer(x))\r\n",
    "            x = F.softplus(layer(x))\r\n",
    "        out = self.layers[-1](x)\r\n",
    "        return out\r\n",
    "    \r\n",
    "    def pretrain(self, training_data,\r\n",
    "            epochs=10,\r\n",
    "            verbose=False,\r\n",
    "            batch_size=500,\r\n",
    "            learning_rate=0.01):\r\n",
    "        if verbose:\r\n",
    "            print('Calculating PCA')\r\n",
    "        pca = torch.tensor(PCA(n_components=2).fit_transform(training_data), dtype=training_data.dtype)\r\n",
    "        \r\n",
    "        dataset = torch.utils.data.TensorDataset(training_data, pca)\r\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\r\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\r\n",
    "        criterion = nn.MSELoss()\r\n",
    "        \r\n",
    "        if verbose:\r\n",
    "            print('{time}: Beginning pretraining for {epochs} epochs'.format(\r\n",
    "                time=datetime.datetime.now(),\r\n",
    "                epochs=epochs))\r\n",
    "        \r\n",
    "        for epoch in range(epochs):\r\n",
    "            running_loss = 0\r\n",
    "            for batch, data in enumerate(dataloader):\r\n",
    "                \r\n",
    "                features, targets = data\r\n",
    "                    \r\n",
    "                if self.use_cuda:\r\n",
    "                    features = features.cuda()\r\n",
    "                    targets = targets.cuda()\r\n",
    "                    \r\n",
    "                optim.zero_grad()\r\n",
    "                \r\n",
    "                loss = criterion(self(features), targets)\r\n",
    "                \r\n",
    "                loss.backward()\r\n",
    "                optim.step()\r\n",
    "                    \r\n",
    "                running_loss += loss.item()\r\n",
    "                \r\n",
    "            if verbose:\r\n",
    "                print('{time}: Loss after epoch {ep}: {rloss}'.format(\r\n",
    "                    time=datetime.datetime.now(),\r\n",
    "                    ep=epoch,\r\n",
    "                    rloss=running_loss))\r\n",
    "\r\n",
    "        if verbose:\r\n",
    "            print('{time}: Finished pretraining'.format(\r\n",
    "                time=datetime.datetime.now()))\r\n",
    "        \r\n",
    "    def fit(self, training_data,\r\n",
    "            p_ij=None,\r\n",
    "            pretrain=False,\r\n",
    "            epochs=10,\r\n",
    "            verbose=False,\r\n",
    "            optimizer=torch.optim.Adam,\r\n",
    "            batch_size=500,\r\n",
    "            learning_rate=0.01):\r\n",
    "        \r\n",
    "        assert training_data.shape[1] == self.input_dim, \"Input training data must be same shape as training `num_inputs`\"\r\n",
    "        \r\n",
    "        self.p_ij = p_ij\r\n",
    "        self._epochs = epochs\r\n",
    "        \r\n",
    "        if pretrain:\r\n",
    "            self.pretrain(training_data, epochs=5, verbose=verbose, batch_size=batch_size)\r\n",
    "            \r\n",
    "        if self.p_ij is None:\r\n",
    "            self.p_ij = p_ij_sym(training_data.numpy(), self.knn, verbose=verbose)\r\n",
    "            \r\n",
    "        dataset = torch.utils.data.TensorDataset(training_data, torch.arange(training_data.shape[0]))\r\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\r\n",
    "        optim = optimizer(self.parameters(), lr=learning_rate)\r\n",
    "        \r\n",
    "        if verbose:\r\n",
    "            print('{time}: Beginning training for {epochs} epochs'.format(\r\n",
    "                time=datetime.datetime.now(),\r\n",
    "                epochs=epochs))\r\n",
    "        \r\n",
    "        for epoch in range(epochs):\r\n",
    "            running_loss = 0\r\n",
    "            for batch, data in enumerate(dataloader):\r\n",
    "                \r\n",
    "                features, indices = data\r\n",
    "                \r\n",
    "                p = submatrix(self.p_ij, indices.numpy())\r\n",
    "                    \r\n",
    "                if self.use_cuda:\r\n",
    "                    features = features.cuda()\r\n",
    "                    p = p.cuda()\r\n",
    "                    \r\n",
    "                optim.zero_grad()\r\n",
    "                \r\n",
    "                q = q_ij(self(features), self.a, self.b)\r\n",
    "                loss = umap_loss(p, q)\r\n",
    "                \r\n",
    "                loss.backward()\r\n",
    "                optim.step()\r\n",
    "                    \r\n",
    "                running_loss += loss.item()\r\n",
    "                \r\n",
    "            if verbose:\r\n",
    "                print('{time}: Loss after epoch {ep}: {rloss}'.format(\r\n",
    "                    time=datetime.datetime.now(),\r\n",
    "                    ep=epoch+1,\r\n",
    "                    rloss=running_loss))\r\n",
    "\r\n",
    "        if verbose:\r\n",
    "            print('{time}: Finished training'.format(\r\n",
    "                time=datetime.datetime.now()))\r\n",
    "            \r\n",
    "    def test(self, training_data,\r\n",
    "            p_ij=None,\r\n",
    "            optimizer=torch.optim.Adam,\r\n",
    "            batch_size=500,\r\n",
    "            learning_rate=0.01):\r\n",
    "        \r\n",
    "        assert training_data.shape[1] == self.input_dim, \"Input training data must be same shape as training `num_inputs`\"\r\n",
    "        \r\n",
    "        self.p_ij = p_ij\r\n",
    "            \r\n",
    "        if self.p_ij is None:\r\n",
    "            self.p_ij = p_ij_sym(training_data.numpy(), self.knn, verbose=True)\r\n",
    "            \r\n",
    "        dataset = torch.utils.data.TensorDataset(training_data, torch.arange(training_data.shape[0]))\r\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\r\n",
    "        optim = optimizer(self.parameters(), lr=learning_rate)\r\n",
    "        \r\n",
    "        for epoch in range(5):\r\n",
    "            running_loss = 0\r\n",
    "            for batch, data in enumerate(dataloader):\r\n",
    "                \r\n",
    "                features, indices = data\r\n",
    "                \r\n",
    "                p = submatrix(self.p_ij, indices.numpy())\r\n",
    "                    \r\n",
    "                if self.use_cuda:\r\n",
    "                    features = features.cuda()\r\n",
    "                    p = p.cuda()\r\n",
    "                    \r\n",
    "                optim.zero_grad()\r\n",
    "                \r\n",
    "                q = q_ij(self(features), self.a, self.b)\r\n",
    "                loss = umap_loss(p, q)\r\n",
    "                \r\n",
    "                return self(features), p, q, loss\r\n",
    "                \r\n",
    "                loss.backward()\r\n",
    "                optim.step()\r\n",
    "                    \r\n",
    "                running_loss += loss.item()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "-----------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mnist = MNIST(\n",
    "    './data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "testdata = mnist.data.reshape(-1, 28*28) / 255."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "foo = Parametric_UMAP(28*28, 2, 100, 0.01, use_cuda=True, hidden_layer_dims=[300,100])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "p_precalc = foo.p_ij"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "foo.fit(testdata[:20000], p_ij=p_precalc, batch_size=500, epochs=20, learning_rate=0.1, pretrain=False, verbose=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bar = foo(testdata[:10000].cuda()).cpu().detach().numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "colors = [plt.cm.tab10.colors[i] for i in mnist.targets[:10000]]\n",
    "ax.scatter(bar[:,0],bar[:,1],c=colors, s=2)\n",
    "recs = []\n",
    "for i in range(0,10):\n",
    "    recs.append(mpatches.Rectangle((0,0),1,1,fc=plt.cm.tab10.colors[i]))\n",
    "ax.legend(recs,list(range(10)),loc=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "foof, foop, fooq, fool = foo.test(testdata[:20000], p_ij=p_precalc, batch_size=500, learning_rate=0.1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "foop.min()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a = foop.detach().cpu()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "b = foof.clone().detach().cpu().requires_grad_(True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "c = q_ij(b, 1.93, 0.79)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "c"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(a/c).fill_diagonal_(0.).min()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "b.grad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a = torch.tensor([[0.5, 0.6], [0.7, 0.3]])\n",
    "b = torch.tensor([[0.1, 0.2], [0.4, 0.3]], requires_grad=True)\n",
    "c = q_ij(b, 1.93, 0.79)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "umap_loss(a, c).backward()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "b.grad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b28ae10d83720f36e64a475947a69598f522ce7ea0d14cd27597289d37d6b09"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}