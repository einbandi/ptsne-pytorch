{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.optimize import root_scalar, curve_fit\n",
    "from pynndescent import NNDescent\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import PCA"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def exp_k(dists, sigma):\n",
    "    return np.exp(- (dists - dists[0]) / sigma).sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def find_sigma(d, k, lower_bound=1e-6, upper_bound=1e6):\n",
    "    return root_scalar(\n",
    "        lambda s: exp_k(d, s) - np.log2(k),\n",
    "        bracket=(lower_bound, upper_bound)\n",
    "    ).root"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def p_ij_sym(x, k, verbose=False):\n",
    "    num_pts = x.shape[0]\n",
    "    if verbose:\n",
    "        print('Indexing')\n",
    "    index = NNDescent(x)\n",
    "    neighbors = np.empty((num_pts, k), dtype=np.int)\n",
    "    p_ij = np.empty((num_pts, k))\n",
    "    for i, xi in enumerate(x):\n",
    "        if verbose:\n",
    "            print('Calculating probabilities: {cur}/{tot}'.format(\n",
    "                cur=i+1, tot=num_pts), end='\\r')\n",
    "        nn, dists = index.query([xi], k+1)\n",
    "        sigma = find_sigma(dists[0, 1:], k)\n",
    "        neighbors[i] = nn[0, 1:]\n",
    "        p_ij[i] = np.exp(- (dists[0, 1:] - dists[0, 1]) / sigma)\n",
    "    row_indices = np.repeat(np.arange(num_pts), k)\n",
    "    p = csr_matrix((p_ij.ravel(), (row_indices, neighbors.ravel())))\n",
    "    return p + p.transpose() - (p.multiply(p.transpose()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def pdiff(x):\n",
    "    batch_size = x.shape[0]\n",
    "    expanded = x.unsqueeze(1)\n",
    "    tiled = torch.repeat_interleave(expanded, batch_size, dim=1)\n",
    "    diffs = tiled - tiled.transpose(0, 1)  \n",
    "    return diffs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def find_ab_params(spread, min_dist):\n",
    "    \"\"\"Fit a, b params for the differentiable curve used in lower\n",
    "    dimensional fuzzy simplicial complex construction. We want the\n",
    "    smooth curve (from a pre-defined family with simple gradient) that\n",
    "    best matches an offset exponential decay.\n",
    "    \"\"\"\n",
    "\n",
    "    def curve(x, a, b):\n",
    "        return 1.0 / (1.0 + a * x ** (2 * b))\n",
    "\n",
    "    xv = np.linspace(0, spread * 3, 300)\n",
    "    yv = np.zeros(xv.shape)\n",
    "    yv[xv < min_dist] = 1.0\n",
    "    yv[xv >= min_dist] = np.exp(-(xv[xv >= min_dist] - min_dist) / spread)\n",
    "    params, covar = curve_fit(curve, xv, yv)\n",
    "    return params[0], params[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def dist_mat_squared(x):\n",
    "    batch_size = x.shape[0]\n",
    "    expanded = x.unsqueeze(1)\n",
    "    tiled = torch.repeat_interleave(expanded, batch_size, dim=1)\n",
    "    diffs = tiled - tiled.transpose(0, 1)\n",
    "    sum_act = torch.sum(torch.pow(diffs,2), axis=2)    \n",
    "    return sum_act"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# more memory-efficient version of distmat\n",
    "def dist_mat_squared_new(x):\n",
    "    n = x.shape[0]\n",
    "    distmat_cond = F.pdist(x)\n",
    "    distmat = torch.zeros((n,n), device=x.device)\n",
    "    a,b = torch.triu_indices(n, n, offset=1)\n",
    "    distmat[[a,b]] = distmat_cond.pow(2)\n",
    "    return distmat + distmat.T"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def q_ij(x, a, b):\n",
    "    dists = dist_mat_squared(x)\n",
    "    dists.fill_diagonal_(1.)\n",
    "    q = torch.pow(1 + a * torch.pow(dists, b), -1)\n",
    "    q.fill_diagonal_(0.)\n",
    "    return q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def submatrix(m, indices):\n",
    "    dim = len(indices)\n",
    "    indices = np.array(np.meshgrid(indices, indices)).T.reshape(-1,2).T\n",
    "    return torch.tensor(m[indices[0], indices[1]].reshape(dim, dim))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def umap_loss(p, q, eps=1.0e-7):\n",
    "    eps = torch.tensor(eps, dtype=p.dtype)\n",
    "    l_a = torch.mul(p, torch.log(p + eps) - torch.log(q + eps))\n",
    "    l_b = torch.mul(1 - p, torch.log(1 - p + eps) - torch.log(1 - q + eps))\n",
    "    loss_mat = l_a #+ l_b\n",
    "    loss_mat.fill_diagonal_(0.)\n",
    "    return torch.sum(loss_mat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Parametric_UMAP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, knn, min_dist,\n",
    "                 spread=1.0,\n",
    "                 hidden_layer_dims=None,\n",
    "                 seed=None,\n",
    "                 use_cuda=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.knn = knn\n",
    "        self.min_dist = min_dist\n",
    "        self.spread = spread\n",
    "        \n",
    "        self.a, self.b = find_ab_params(self.spread, self.min_dist)\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # If no layers provided, use the same architecture as van der maaten 2009 paper\n",
    "        if hidden_layer_dims is None:\n",
    "            hidden_layer_dims = [500, 500, 2000]\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        cur_dim = input_dim\n",
    "        for hdim in hidden_layer_dims:\n",
    "            self.layers.append(nn.Linear(cur_dim, hdim))\n",
    "            cur_dim = hdim\n",
    "        self.layers.append(nn.Linear(cur_dim, output_dim))\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            self.cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            # x = torch.sigmoid(layer(x))\n",
    "            x = F.softplus(layer(x))\n",
    "        out = self.layers[-1](x)\n",
    "        return out\n",
    "    \n",
    "    def pretrain(self, training_data,\n",
    "            epochs=10,\n",
    "            verbose=False,\n",
    "            batch_size=500,\n",
    "            learning_rate=0.01):\n",
    "        if verbose:\n",
    "            print('Calculating PCA')\n",
    "        pca = torch.tensor(PCA(n_components=2).fit_transform(training_data), dtype=training_data.dtype)\n",
    "        \n",
    "        dataset = torch.utils.data.TensorDataset(training_data, pca)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        if verbose:\n",
    "            print('{time}: Beginning pretraining for {epochs} epochs'.format(\n",
    "                time=datetime.datetime.now(),\n",
    "                epochs=epochs))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0\n",
    "            for batch, data in enumerate(dataloader):\n",
    "                \n",
    "                features, targets = data\n",
    "                    \n",
    "                if self.use_cuda:\n",
    "                    features = features.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                    \n",
    "                optim.zero_grad()\n",
    "                \n",
    "                loss = criterion(self(features), targets)\n",
    "                \n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                    \n",
    "                running_loss += loss.item()\n",
    "                \n",
    "            if verbose:\n",
    "                print('{time}: Loss after epoch {ep}: {rloss}'.format(\n",
    "                    time=datetime.datetime.now(),\n",
    "                    ep=epoch,\n",
    "                    rloss=running_loss))\n",
    "\n",
    "        if verbose:\n",
    "            print('{time}: Finished pretraining'.format(\n",
    "                time=datetime.datetime.now()))\n",
    "        \n",
    "    def fit(self, training_data,\n",
    "            p_ij=None,\n",
    "            pretrain=False,\n",
    "            epochs=10,\n",
    "            verbose=False,\n",
    "            optimizer=torch.optim.Adam,\n",
    "            batch_size=500,\n",
    "            learning_rate=0.01):\n",
    "        \n",
    "        assert training_data.shape[1] == self.input_dim, \"Input training data must be same shape as training `num_inputs`\"\n",
    "        \n",
    "        self.p_ij = p_ij\n",
    "        self._epochs = epochs\n",
    "        \n",
    "        if pretrain:\n",
    "            self.pretrain(training_data, epochs=5, verbose=verbose, batch_size=batch_size)\n",
    "            \n",
    "        if self.p_ij is None:\n",
    "            self.p_ij = p_ij_sym(training_data.numpy(), self.knn, verbose=verbose)\n",
    "            \n",
    "        dataset = torch.utils.data.TensorDataset(training_data, torch.arange(training_data.shape[0]))\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        optim = optimizer(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(training_data, torch.arange(training_data.shape[0]))\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        optim = optimizer(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        if verbose:\n",
    "            print('{time}: Beginning training for {epochs} epochs'.format(\n",
    "                time=datetime.datetime.now(),\n",
    "                epochs=epochs))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0\n",
    "            for batch, data in enumerate(dataloader):\n",
    "                \n",
    "                features, indices = data\n",
    "                \n",
    "                p = submatrix(self.p_ij, indices.numpy())\n",
    "                    \n",
    "                if self.use_cuda:\n",
    "                    features = features.cuda()\n",
    "                    p = p.cuda()\n",
    "                    \n",
    "                optim.zero_grad()\n",
    "                \n",
    "                q = q_ij(self(features), self.a, self.b)\n",
    "                loss = umap_loss(p, q)\n",
    "                \n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                    \n",
    "                running_loss += loss.item()\n",
    "                \n",
    "            if verbose:\n",
    "                print('{time}: Loss after epoch {ep}: {rloss}'.format(\n",
    "                    time=datetime.datetime.now(),\n",
    "                    ep=epoch+1,\n",
    "                    rloss=running_loss))\n",
    "\n",
    "        if verbose:\n",
    "            print('{time}: Finished training'.format(\n",
    "                time=datetime.datetime.now()))\n",
    "            \n",
    "    def test(self, training_data,\n",
    "            p_ij=None,\n",
    "            optimizer=torch.optim.Adam,\n",
    "            batch_size=500,\n",
    "            learning_rate=0.01):\n",
    "        \n",
    "        assert training_data.shape[1] == self.input_dim, \"Input training data must be same shape as training `num_inputs`\"\n",
    "        \n",
    "        self.p_ij = p_ij\n",
    "            \n",
    "        if self.p_ij is None:\n",
    "            self.p_ij = p_ij_sym(training_data.numpy(), self.knn, verbose=True)\n",
    "            \n",
    "        dataset = torch.utils.data.TensorDataset(training_data, torch.arange(training_data.shape[0]))\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        optim = optimizer(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        for epoch in range(5):\n",
    "            running_loss = 0\n",
    "            for batch, data in enumerate(dataloader):\n",
    "                \n",
    "                features, indices = data\n",
    "                \n",
    "                p = submatrix(self.p_ij, indices.numpy())\n",
    "                    \n",
    "                if self.use_cuda:\n",
    "                    features = features.cuda()\n",
    "                    p = p.cuda()\n",
    "                    \n",
    "                optim.zero_grad()\n",
    "                \n",
    "                q = q_ij(self(features), self.a, self.b)\n",
    "                loss = umap_loss(p, q)\n",
    "                \n",
    "                return self(features), p, q, loss\n",
    "                \n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                    \n",
    "                running_loss += loss.item()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "-----------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mnist = MNIST(\n",
    "    './data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "testdata = mnist.data.reshape(-1, 28*28) / 255."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "foo = Parametric_UMAP(28*28, 2, 100, 0.01, use_cuda=True, hidden_layer_dims=[300,100])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "p_precalc = foo.p_ij"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "foo.fit(testdata[:20000], p_ij=p_precalc, batch_size=500, epochs=20, learning_rate=0.1, pretrain=False, verbose=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bar = foo(testdata[:10000].cuda()).cpu().detach().numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "colors = [plt.cm.tab10.colors[i] for i in mnist.targets[:10000]]\n",
    "ax.scatter(bar[:,0],bar[:,1],c=colors, s=2)\n",
    "recs = []\n",
    "for i in range(0,10):\n",
    "    recs.append(mpatches.Rectangle((0,0),1,1,fc=plt.cm.tab10.colors[i]))\n",
    "ax.legend(recs,list(range(10)),loc=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "foof, foop, fooq, fool = foo.test(testdata[:20000], p_ij=p_precalc, batch_size=500, learning_rate=0.1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "foop.min()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a = foop.detach().cpu()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "b = foof.clone().detach().cpu().requires_grad_(True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "c = q_ij(b, 1.93, 0.79)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "c"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(a/c).fill_diagonal_(0.).min()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "b.grad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a = torch.tensor([[0.5, 0.6], [0.7, 0.3]])\n",
    "b = torch.tensor([[0.1, 0.2], [0.4, 0.3]], requires_grad=True)\n",
    "c = q_ij(b, 1.93, 0.79)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "umap_loss(a, c).backward()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "b.grad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76b22f2d16ad6fe83001a338aacb6eece0f02ecda19b2433097d5d6b62848fa1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('ml': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}